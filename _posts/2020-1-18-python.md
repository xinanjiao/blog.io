---
layout: post
title: python爬虫之--静态网站和动态网站的爬取
date: 2020-1-18
categories: blog
tags: [python爬虫]
description: 语言
---

### 简述一下今天的小激动
<p style="color: red">哈哈哈，今天着实开心，踩着年关完成了我一直想完成的一件事-----就是给我博客加上评论系统。创建这个博客以来，我一直觉得很遗憾的就是没有评论板块，今天完成了，也算是完成了我的一件心事。还有就是终于不用“偷偷摸摸写爬虫了”，终于把破解版永久pycharm给搞到了，想不到原来就是把一个文件放错位置才导致失败，哈哈哈</p>


## 静态网页的爬取
### requests
requests库是个功能强大的库对象。通过内置get('url')可以得到网页的html信息。
      
     r=requests.get(link,headers=header)
     print(r.text)

上述代码中，link是目标网页的地址，header为伪浏览器，也叫做请求头。<br>
#### 请求头做法
打开目标网页，通过审查元素打开网页操作台，然后点击network,按F5刷新，点击有一个网址的文件，看右下角有一个'user-angent',这个就是请求头，一般的右下角还有'host：'这个选项，一般都要加上。但它一般都是主网址。
![python](/img/python2.png)

#### 状态响应码
即r.statue_code。<br>
详细可看这篇博客。<https://blog.csdn.net/ddhsea/article/details/79405996>

#### 网页解析--beautifulsoup
通过上面的代码得到的是整个网页的信息，而我梦要提取我们想要的，所以要用到beautifulsoup,这个库我还没有了解，所知也甚微，但这句就是转换为beautifusoup类型的语句

    soup=beautifulsoup(r.text,"lxml")
然后用beautiful库自带的操作查找我们想要的标签，比如'div','p'等等

#### 实例
爬取一个豆瓣的最近电影前top100<br>
按照上述的准备好之后就去原网站去看我们想要的div或着p标签了。为了简单我就爬一个页面叭（狗头.jpg)。代码也比较简单

    #!/usr/bin/env python
    # -*-coding:utf-8-*-
    import requests
    from bs4 import BeautifulSoup
    import re
    header={
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36',
    'Host':'movie.douban.com'
    }
    movie_list=[]
    link='https://movie.douban.com/chart'
    r=requests.get(link,headers=header,timeout=10)
    print('状态响应码为',r.status_code)
    #print(r.text)
    soup=BeautifulSoup(r.text,"lxml")
    div_list=soup.find_all('div',class_='pl2')
    for i in div_list:
    	movie=i.a.text.strip()#a标签下的span标签,strip为字符串分割
    	movie_list.append(movie)
    for i in range(0,len(movie_list)):
    	print(movie_list[i])


结果
![python](/img/python3.png)

## 动态网页的爬取
什么叫做动态网页？
<p style="color: red">静态网页和动态网页最大的区别就是：静态网页在点击每个按钮后网址会发生变化，这种叫做跳转，有了变化的网址那就可以利用网址爬取。动态网站就不是。例如淘宝评论的时候，你发一条评论，它的网址不会变化，这是由于javascript的语言引起的，它由前端传输数据，你没法定位到网址的变化</p>

对待这种情况，我们有两种办法：<br>
1. 尽力找到js传输文件，解析真实地址抓取，因为是json文件，要用到json语法。抓取就和静态抓取差不多了

2. 运用selenium模拟浏览器抓取（真.高大上），可以模拟人一样由机器人打开浏览器，因为数据都渲染在html代码中，所以直接分析css即可。selenium用处很多，还可以命令浏览器自己输入登陆密码等。

### 解析地址抓取
和抓取静态的页面差不多，也是找到目标网址，取得页面代码，分析代码。只不过静态页面抓取回来的是html代码，而动态抓取回来的是json代码，分析不同。<br>

其中最难搞的是找到那个真实地址。<br>
以网上随便的一个资讯为例，我们主要看评论部分，因为它是动态的。<br>
对着评论打开后台，审查元素，搜索js文件，一般评论名都叫comment后者query之类的。如图，我就找到了这个文件，文件名字就是网址。
![python](/img/python5.png)
![python](/img/python4.png)



代码还是挺简单的，就是加入了一些json的语法，慢慢来。

    #!/usr/bin/env python
    # -*-coding:utf-8-*-
    import requests
    from bs4 import BeautifulSoup
    import json
    import re
    header={
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36',
    }
    movie_list=[]
    link='https://u.api.look.360.cn/comment/lists?callback=jQuery19107540319669458282_1579351379994&type=1&page_key=&page=1&from=0&client_id=15&start_date=&url=http%3A%2F%2Fzm.news.so.com%2F140a4cf472c87442dd860781f71611fd&pid=&sub_limit=5&num=5&src=chrome&f=jsonp&scheme=https&_=1579351379995'
    r=requests.get(link,headers=header,timeout=10)
    print('状态响应码为',r.status_code)
    jstring=r.text;
    jstring=jstring[jstring.find('{'):-2]#从第一个大括号提取，最后两个字符和括号分号不取
    jdata=json.loads(jstring)
    commentlist=jdata['data']['comments']
    for i in commentlist:
    	print(i['message'])


结果
![python](/img/python6.png)


### 通过selenium模拟浏览器抓取
<p style="color: red">上面找真实地址的过程是很痛苦的，我作为亲身体验者，感到很痛苦。这个模拟浏览器抓取就是远程控制浏览器按照脚本代码做出单击，输入，打开，验证等操作（怪不得现在登陆操作越来越复杂，就是防爬虫滴！）</p>


selenium可以选择打开IE,Firfox,Google,Chrome等<br>

    from selenium import webdriver
    driver.webdriver.Firfox()
    driver.get(url)

这个是引入selenium并且打开火狐浏览器，但会报错，所以我根据书上的解法，下载了一个压缩包，更改路径为：

    driver=webdriver.Firefox(executable_path=r'C:\Users\92507\Desktop\geckodriver.exe')

这样就行了，就可以通过火狐浏览器远程打开某网址，进而进行操作<br>

先了解一下selenium选择元素的方式:<https://blog.csdn.net/qq_32897143/article/details/80383502><br>

但最常用的是find_element_by_css_selector()和find_element_by_xpath()<br>

其他的内容请看这博客：<http://www.santostang.com/2018/07/15/4-3-%E9%80%9A%E8%BF%87selenium-%E6%A8%A1%E6%8B%9F%E6%B5%8F%E8%A7%88%E5%99%A8%E6%8A%93%E5%8F%96/><br>


### 来个实例叭:)  爬取著名博主matrix67博客评论
![python](/img/python7.png)
上图为博客的原评论，我们的任务是将它爬取过来，按照上述操作就可简单爬取。

    import requests
    import json
    from bs4 import BeautifulSoup
    import re
    from selenium import webdriver
    from selenium.webdriver.common.keys import Keys
    import time
    header={
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'
    }
    driver=webdriver.Firefox(executable_path=r'C:\Users\92507\Desktop\geckodriver.exe')
    driver.implicitly_wait(20)
    driver.get('http://www.matrix67.com/blog/archives/6947')
    time.sleep(10)
    i=driver.find_elements_by_css_selector('div.comment-text')
    for k in i:
    	s=k.find_element_by_tag_name('p')
    	print(s.text)

结果
![python](/img/python8.png)

书上的例子更难，涉及翻页点击。我爬这个网页就超时了，后面还要学，如何限制css防止超时。<br>
为什么要time.sleep()呢？<br>
因为防止爬太快被网页禁止访问，很多网页都很聪明，比如淘宝，我爬个数据，现在的水平完全不行（太难了）