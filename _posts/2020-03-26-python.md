---
layout: post
title: python爬取京东华为荣耀手机500条评论，然后做成词云分析频繁词
date: 2020-03-26
categories: blog
tags: [python爬虫]
description: 文章金句。
---
参考网址：<https://blog.csdn.net/u014044812/article/details/95198791>
### 为什么我要写这个爬虫
在b站学习python爬虫慕课的时候，有一个实例就是爬取淘宝的商品，然后那时候老师的视频是17年录的，那时候淘宝的反爬机制没有那么严格。而现在自动登陆贼麻烦，我在网上看淘宝登陆的时候看见了京东的这个爬虫，京东相比来说比较好爬，我就跟着教程学习了。有一说一，python的库可真好用。

### 准备工作
1. 进入京东的页面，开启控制台打开network获取json格式的评论文件。
2. 直接爬取该json文件的url，发现并没有结果，打开控制台复制一下请求头继续操作。
3. 多页爬取，将评论存在txt文件里。
4. 运用jieba库，进行分词
5. 运用wordcloud库生成词云


### 注意
json文件的操作需要运用python内置的json解析库。

爬取的时候，注意京东是'GBK'编码，然后写入文件是要加上encoding='utf-8'。爬取的时候为了防止频率过快而被封ip，加上睡眠时间time.sleep(random.random() * 5)

取分词的时候，注意要去掉中文的符号以及数字和字母

制作词云的时候，打开图片要用Image.open('XXX')

## 各个板块

### 爬取评论
取得评论内容：因为是动态页面，爬取动态页面的数据有两个办法：第一个是找到json文件加以解析，第二个是用selnium模拟浏览器操作。我这里用前者。
```
def getcomments():#获得评论
	#os是一个检查文件等输入输出的库
    if os.path.exists(root):  # 如果存在就清空
        os.remove(root)
    try:

        for j in range(0, 50):
            link = str1 + linkadd(j) + str2 #找到规律然后添加上
            # print(link)
            r = requests.get(link, headers=kv)
            r.raise_for_status()
            r.encoding = r.apparent_encoding
            r_json = r.text[20:-2] #去掉非法格式
            r_string = json.loads(r_json)
            r_comments = r_string['comments']
            for i in r_comments:
                with open(root, 'a+', encoding='utf-8') as f:  # a+覆盖写入
                    f.write(i['content'])
            print('第 %d个评论爬取成功' % j)
            time.sleep(random.random() * 5)
    except:
        print("sorry! crawl fail")
```

### 切割分词
这里用到了jieba库，一个优秀的第三方库，里面很多的参数需要我好好的去学习一下，就是把句子切割为一个个的词语，为做词云做准备。
```
def cutword():#分词
    with open(root,encoding='utf-8') as f:
        comments=f.read()
        #comments = re.sub(r"[0-9\s+\.\!\/_,$%^*()?;；:-【】+\"\']+|[+——！，;：。？、~@#￥%……&*（）]+", "", comments)
        for ch in '!"#$%&()*+,-./:;：，！<=>?@[\\]^_‘？{|}【】￥~0123456789。':#去除非法字符
            comments = comments.replace(ch, " ")
        comments=re.sub('[A-Za-z]','',comments) #去除英文字母
        newstring=jieba.cut(comments,cut_all=True) #全部切割 返回迭代类型
        wl=' '.join(newstring)
        #print(wl)
        return wl

```


### 生成词云
生成词云需要很多第三方库的配合，需要下载：numpy pillow,wordcloud和图像处理的matplotlib。<br>
安装worldcloud库也是一个苦差事，最后Anaconda也没有安装好，还是看了网上的一个通用方法。<br>
```
#制作词云
def creativewordcloud():
    coloring=np.array(Image.open('girl.png'))
    wordpath='simsun.ttc'
    #配置参数
    wc=WordCloud(width=1000,height=700,background_color='white',max_words=2000,mask=coloring,
                 scale=4,max_font_size=50,random_state=42,font_path=wordpath)
    #生成词云
    wc.generate(cutword())
    #生成图片
    plt.imshow(wc,interpolation="bilinear")
    plt.axis('off')
    plt.figure()
    plt.show()
    wc.to_file('C:\\Users\\92507\\Desktop\\temp.png')
```






