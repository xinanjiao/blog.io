---
layout: post
title: 对爬虫框架scrapy的认识
date: 2020-02-09
categories: blog
tags: [python爬虫]
description: 语言
---

### 对scrapy的认识
先上一波网上官方的介绍（这可以让我更好的认识每个框架）：<br>
Scrapy一个开源和协作的框架，其最初是为了页面抓取（更确切来说，网络抓取）所设计的，使用它可以快速、简单、可扩展

的方式从网站中提取所需的数据。但目前Scrapy的用途十分广泛，可用于如挖掘、监测和自动化测试等领域，也可以应用在API

所返回的数据（例如Amazon Associates Web Services）或者通用的网络爬虫。

      Scrapy是基于twisted框架开发而来，twisted是一个流行的事件驱动的python网络框架。因此Scrapy使用了一种非阻塞（又名异步）的代码来实现并发。整体架构大致如下
![scrapy](/img/scrapy.png)

Scrapy数据流是由执行的核心引擎（engine）控制，流程是这样的：

1. 引擎打开一个网站（open adomain）,找到处理该网站的Spider并向该spider请求第一个要抓取的URL（s）。

2. 引擎从Spider中获取到第一个要抓取的URL并在调度器（Scheduler）以Request调度。

3. 引擎向调度器请求下一个要爬取的URL。

4. 调度器返回下一个要抓取的URL给引擎，引擎将URL通过下载中间件（请求（request）方向）转发给下载器（Downloader）.

5. 一旦页面下载完毕，下载器生成一个该页面的Response,并将其通过下载中间件（返回（response）方向）发送给引擎。

6. 引擎从下载器中接收到Response并通过Spider中间件（输入方向）发送给Spider处理。

7. Spider处理Response并返回爬取到的Item给Item Pipeline,将（Spider返回的）Request给调度器。

8. 引擎将（Spider返回的）爬取的Item给Item Pipeline,将（Spider返回的）Request给调度器。

9. （从第二步）重复直到调度器中没有更多地request,引擎关闭该网站


### 谈谈我的理解
spiders需要自己写，里面有要爬的url，和自己写的解析网站的代码，我个人喜欢bs4。<br>
spiders把url传给爬虫引擎，爬虫引擎把这个url交个调度器调度,调度器返回request个爬虫引擎。最后引擎把这个request给download。download把从internet上面爬取到的内容以response的的形式通过引擎返回给spiders，spiders再加以解析，最后再通过引擎给item pipline，也就是一个储存管道，一般用它来储存东西。<br>

### 总结一下吧
需要自己写的内容就是：<br>
item.py<br>

spiders.py(自己创建的文件）<br>

pipline.py(储存管道)<br>


### 命令
	scrapy startproject [项目名字]

通过cmp在桌面创建一个爬虫文件夹

    scrapy genspider [项目名字] [项目网址]

创建一个目标网站的爬虫项目，在里面编写解析代码


之后就是解析及储存操作啦！



