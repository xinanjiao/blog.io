---
layout: post
title: 爬取豆瓣《何以为家》前1000条影评，并做成词云分析关键字
date: 2020-03-27
categories: blog
tags: [python爬虫]
description: 语言
---

### 准备
1. 爬取前一般都先看一下目标网站的robots文件，不过……我懂得，那只是好人文件，你想爬，它是拦不住你的
2. 先试探试探目标页面是否有反爬措施，如：字体加密，表头限制，‘放毒’，登陆验证等。试了一下，直接就可以爬。表头不加refere都可以。
3. 批量爬取，观察目标url，发现和京东差不多唉，初始页面page=0,下一页就是page=20,以此类推，也就是一个页面有20条评论，然后变换这个数字就是下一页。
4. 测试批量爬取，然后储存到txt文件。爬到第11个的时候返回编码错误，和昨天京东的差不多，然后我在文件读入那里加上了‘encoding='utf-8'’，然后可以写进去了，但乱码了，最后百度了一下，去掉r.encoding=r.apparent_encoding，然后编码改为encoding='gb18030'(gb18030是中文简体的一种编码方式)。然后写入成功并成功编码。
5. 批量爬取返沪错误。然后我看了一下豆瓣网站，也就是当页数到page=220时就不能爬取了，就是只能登陆才能爬取。这也是一种很巧妙的反爬措施。解决方法：观察豆瓣登陆页面，找到原来的登陆页面，找到data提交位置，运用requests.Session()对象，该对象会记录该网站的cookie。然后使用s.get(url,headers)等功能时就可以不用登陆了。
6. jieba分词。运用jieba分词库实现对评论的分词
7. wordcloud实现词云。主要是熟悉wordcloud的代码和plt的参数运用。和京东的一样。

## 步骤

### 登陆豆瓣，保持登陆状态（保留cookie)
```
def login_douban():
    link='https://accounts.douban.com/j/mobile/login/basic'
    headers = {'user-agent': 'Mozilla/5.0',
                'Referer': 'https://accounts.douban.com/passport/login?source=movie'
               }
    data={
        'name':'153104xxxxx',
        'password':'zzhlzfxxxxxxx',
        'remember':'false'
    }
    try:
        r=s.post(link,headers=headers,data=data)
        r.raise_for_status()
    except:
        print('登陆失败')
    print('登陆成功！')
```

### 批量爬取评论并保存为txt文件
因为豆瓣的评论不是动态的，爬取页面时也可以返回评论的内容，所以我采用了bs4库进行解析(re貌似更简单)。
```
def getrequest():
    try:
        #root='C:\\Users\\92507\\Desktop\\doubancomments.txt'
        link1='https://movie.douban.com/subject/30170448/comments?start='
        link2='&limit=20&sort=new_score&status=P'
        headers={'user-agent':'Mozilla/5.0',
                 #'Referer': 'https: // accounts.douban.com / passport / login?source = movie'
                 }
        # 存在文件即清除
        if os.path.exists(root):
            os.remove(root)
        #爬取前50页评论
        for j in range(0,50):
            link=link1+str(j*20)+link2
            #print(link)
            r=s.get(link,headers=headers)
            r.raise_for_status()
            #r.encoding=r.apparent_encoding
            soup=BeautifulSoup(r.text,"lxml")
            comments=soup.find_all("div",class_='comment')
            for i in comments:
                string=i.p.text
                with open(root,'a',encoding='gb18030') as f:
                    f.write(string)
            #休眠时间不能爬太快
            time.sleep(random.random()*5)
            print('第 %d 页评论爬取完毕！' %j)
    except:
        print('crawl woring!')
```
### jieba库进行分词
步骤即去掉非法字符和昨天一样
```
#用jieba切割分词
def cutword():
    with open(root,encoding='gb18030') as f:
        comments = f.read()
        for ch in '!"#$%&()*+,-./:;：，！<=>?@[\\]^_‘？{|}【】￥~0123456789。（）“”《》<>……；·':  # 去除非法字符
            comments = comments.replace(ch, " ")
        comments=re.sub('[A-Za-z]','',comments) #去除英文字母
        wl1=jieba.cut(comments)
        wl=' '.join(wl1)
        return wl
```
### wordcloud库生成词云
```
#生成词云
def wordcloud():
    #打开图片
    img=np.array(Image.open('girl.png'))
    #配置参数
    font='simsun.ttc' #字体路径
    wc=WordCloud(width=1000,height=700,font_path=font,scale=4,random_state=42,
                 mask=img,max_words=200,max_font_size=50,background_color='white')
    #生成词云
    wc.generate(cutword())
    #生成图片
    plt.imshow(wc,interpolation='bilinear')
    plt.axis('off')# 关闭坐标轴
    plt.figure()
    plt.show()
    wc.to_file(outputroot)
```

### 完整代码

```
#!/usr/bin/env python
# -*-coding:utf-8-*-
import requests
import re
from wordcloud import WordCloud
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import random
import time
from bs4 import BeautifulSoup
import os
import jieba
#requsts session对象保存cookie
s=requests.Session()
root='C:\\Users\\92507\\Desktop\\doubancomments.txt'
outputroot='C:\\Users\\92507\\Desktop\\豆瓣.jpg'
#用jieba切割分词
def cutword():
    with open(root,encoding='gb18030') as f:
        comments = f.read()
        for ch in '!"#$%&()*+,-./:;：，！<=>?@[\\]^_‘？{|}【】￥~0123456789。（）“”《》<>……；·':  # 去除非法字符
            comments = comments.replace(ch, " ")
        comments=re.sub('[A-Za-z]','',comments) #去除英文字母
        wl1=jieba.cut(comments)
        wl=' '.join(wl1)
        return wl

#生成词云
def wordcloud():
    #打开图片
    img=np.array(Image.open('girl.png'))
    #配置参数
    font='simsun.ttc' #字体路径
    wc=WordCloud(width=1000,height=700,font_path=font,scale=4,random_state=42,
                 mask=img,max_words=200,max_font_size=50,background_color='white')
    #生成词云
    wc.generate(cutword())
    #生成图片
    plt.imshow(wc,interpolation='bilinear')
    plt.axis('off')# 关闭坐标轴
    plt.figure()
    plt.show()
    wc.to_file(outputroot)
#登陆豆瓣
def login_douban():
    link='https://accounts.douban.com/j/mobile/login/basic'
    headers = {'user-agent': 'Mozilla/5.0',
                'Referer': 'https://accounts.douban.com/passport/login?source=movie'
               }
    data={
        'name':'15310451436',
        'password':'zzhlzf20000408',
        'remember':'false'
    }
    try:
        r=s.post(link,headers=headers,data=data)
        r.raise_for_status()
    except:
        print('登陆失败')
    print('登陆成功！')
# 爬取评论生成文件
def getrequest():
    try:
        #root='C:\\Users\\92507\\Desktop\\doubancomments.txt'
        link1='https://movie.douban.com/subject/30170448/comments?start='
        link2='&limit=20&sort=new_score&status=P'
        headers={'user-agent':'Mozilla/5.0',
                 #'Referer': 'https: // accounts.douban.com / passport / login?source = movie'
                 }
        # 存在文件即清除
        if os.path.exists(root):
            os.remove(root)
        #爬取前50页评论
        for j in range(0,50):
            link=link1+str(j*20)+link2
            #print(link)
            r=s.get(link,headers=headers)
            r.raise_for_status()
            #r.encoding=r.apparent_encoding
            soup=BeautifulSoup(r.text,"lxml")
            comments=soup.find_all("div",class_='comment')
            for i in comments:
                string=i.p.text
                with open(root,'a',encoding='gb18030') as f:
                    f.write(string)
            #休眠时间不能爬太快
            time.sleep(random.random()*5)
            print('第 %d 页评论爬取完毕！' %j)
    except:
        print('crawl woring!')

if __name__=='__main__':
    #login_douban()
    #getrequest()
    #cutword()
    wordcloud()
```






