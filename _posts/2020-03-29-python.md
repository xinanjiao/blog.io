---
layout: post
title: 爬取关于美国疫情微博评论并做成词云
date: 2020-03-29
categories: blog
tags: [python爬虫]
description: 语言
---

### hello .de
今天做了很久就该做的事，就是换域名。换好了还是和去年4月份第一次配置好.online一样激动。

再见，.online 。你好，.de

### 准备
最近美国疫情真的狂增猛涨，好像昨天就增加了2万人！<br>
我真的鄙视美国的某些政客不管国内的疫情，反而制造舆论带节奏，企图极其民族情绪。他们脑子里只有资本！反而看低人命！<br>
但不管怎么说，我们都生在一个地球，咱们不谈政治，不搞舆论话，说真的，美国的平民不管怎么说都是无辜的，但不乏也有被带节奏的喷子。等到他们患病就喷不出来了吧！<br>
我看最近的新闻，川普怕是在耍川剧变脸哦，果然将党的使命牢记于心。<br>
[段子]在白宫的总统办公室里，特朗普四处看了看，发现没人，便小心的打开了抽屉，在抽屉的夹层里抽出来了一个有红布包裹的盒子。特朗普白眼圈的眼睛眯了一下，放下了抽到一半的雪茄，小心翼翼的拿下了包裹在盒子外面的红布，打开了这个年代久远的檀香盒子，里面有一张照片和一封署名为川建国的信。特朗普拿起了小时候他在天安门照的照片，心里感慨良多。心里默念道：放心，党交给的任务一定完成！！！

哈哈哈。扯多了。最近看见了下图微博，看评论还挺多，我就想看看大多数网民对此事的态度
![03291](/img/03291.png)




网上了解到微博的网页版的反爬程度不亚于淘宝，遂放弃网页版的爬取。<br>
了解到微博居然在电脑上有手机版，网址：https://www.weibo.cn/，而且页面简单，少辽很多css和js，我就试了试，居然评论都不是动态的，也就是说就可以直接爬取页面，解析评论就行了。<br>
有一点就是，未登录的话，只能爬取前5条评论，也就是说要模拟登陆微博，不过手机版微博的登陆形式跟豆瓣实际上一样，去找到原始登陆地址，保留cookie，登陆就行。

### 思路和遇到的问题解决
1. 首先模拟登陆微博，方式和登陆豆瓣一样
2. 登陆后，观察每一页评论url的api，然后批量爬取。不过第一次爬取的时候是乱码的，更改encoding就行了。
3. 保存在txt文件后，用jieba分词切割为词语
4. wordcloud库做成词云

## 步骤

### 登陆微博

```
def login_weibo():
    link="https://passport.weibo.cn/sso/login"
    headers={
        'User-agent':'Mozilla/5.0',
        'Referer':'https://passport.weibo.cn/signin/login?entry=mweibo&r=https%3A%2F%2Fweibo.cn%2F%3Fsudaref%3Dlogin.sina.com.cn%26display%3D0%26retcode%3D6102&backTitle=%CE%A2%B2%A9&vt=&sudaref=weibo.cn&display=0&retcode=6102'
    }
    data={
        'username':'1531045xxxx',
        'password':'zzhlzf20xxxxx',
        'entry': 'weibo',
        'savestate': '1',
        'mainpageflag':'1'
    }
    try:
        r=s.post(link,headers=headers,data=data)
        r.raise_for_status()
    except:
        print('登陆失败')
        return 0
```

### 批量爬取，并存于txt文件
```
#得到评论
def get_comments():
    #如果存在文件 移除
    if os.path.exists(root):
        os.remove(root)
    headers={
        'User-agent':'Mozilla/5.0',
    }
    for j in range(1,50):
        link='https://weibo.cn/comment/IAx26EVe9?uid=5626136031&rl=1&rand=850778&page='
        link1=link+str(j)
        try:
            r=s.get(link1,headers=headers)
            r.raise_for_status()
            #r.encoding=r.apparent_encoding
            soup=BeautifulSoup(r.text,'lxml')
            #print(soup.prettify())
            span=soup.find_all('span',class_='ctt')
            for i in span:
                with open(root,'a',encoding='utf-8') as f:
                    f.write(i.text+'\n')
        except:
            print('crawl woring!')
        print('第 %d 篇评论爬取完成' %j)
        time.sleep(random.random()*5)
```

### jieba分词切割

```
#jieba分词
def cutword():
    with open(root,encoding='utf-8') as f:
        string=f.read()
        for ch in'!"#$%&()*+,-./:;：，！<=>?@[\\]^_‘？{|}【】￥~0123456789。（）“”《》<>……；·~':
            string=string.replace(ch,' ')
        string=re.sub('[A-Za-z]','',string)
        wl=jieba.cut(string)
        wl=' '.join(wl)
       # print(wl)
        return wl
```

### 生成词云

```
#生成词云
def ciyun():
    path='C:\\Users\\92507\\Desktop\\weibo.jpg'
    font='simsun.ttc'
    img='boy.jpg'
    mask=np.array(Image.open(img))
    #词云参数配置
    stopword=['回复','还有','不是','这个','那么','可以']
    wl=WordCloud(width=1000,height=700,scale=4,random_state=42,mask=mask,
                 max_words=200,max_font_size=50,font_path=font,background_color='white',stopwords=stopword)
    wl.generate(cutword())
    plot.imshow(wl,interpolation="bilinear")
    plot.axis('off')
    plot.figure()
    plot.show()
    wl.to_file(path)
```

### 完整代码及成果

```
#!/usr/bin/env python
# -*-coding:utf-8-*-
import re
import time
import requests
from bs4 import BeautifulSoup
import  json
import os
import time
import random
import jieba
import numpy as np
import matplotlib.pyplot as plot
from wordcloud import WordCloud
from PIL import Image
s=requests.Session()
root="C:\\Users\\92507\\Desktop\\weibocomments.txt"
#jieba分词
def cutword():
    with open(root,encoding='utf-8') as f:
        string=f.read()
        for ch in'!"#$%&()*+,-./:;：，！<=>?@[\\]^_‘？{|}【】￥~0123456789。（）“”《》<>……；·~':
            string=string.replace(ch,' ')
        string=re.sub('[A-Za-z]','',string)
        wl=jieba.cut(string)
        wl=' '.join(wl)
       # print(wl)
        return wl

#生成词云
def ciyun():
    path='C:\\Users\\92507\\Desktop\\weibo.jpg'
    font='simsun.ttc'
    img='boy.jpg'
    mask=np.array(Image.open(img))
    #词云参数配置
    stopword=['回复','还有','不是','这个','那么','可以']
    wl=WordCloud(width=1000,height=700,scale=4,random_state=42,mask=mask,
                 max_words=200,max_font_size=50,font_path=font,background_color='white',stopwords=stopword)
    wl.generate(cutword())
    plot.imshow(wl,interpolation="bilinear")
    plot.axis('off')
    plot.figure()
    plot.show()
    wl.to_file(path)
#登陆微博
def login_weibo():
    link="https://passport.weibo.cn/sso/login"
    headers={
        'User-agent':'Mozilla/5.0',
        'Referer':'https://passport.weibo.cn/signin/login?entry=mweibo&r=https%3A%2F%2Fweibo.cn%2F%3Fsudaref%3Dlogin.sina.com.cn%26display%3D0%26retcode%3D6102&backTitle=%CE%A2%B2%A9&vt=&sudaref=weibo.cn&display=0&retcode=6102'
    }
    data={
        'username':'15310451436',
        'password':'zzhlzf20000408',
        'entry': 'weibo',
        'savestate': '1',
        'mainpageflag':'1'
    }
    try:
        r=s.post(link,headers=headers,data=data)
        r.raise_for_status()
    except:
        print('登陆失败')
        return 0
#得到评论
def get_comments():
    #如果存在文件 移除
    if os.path.exists(root):
        os.remove(root)
    headers={
        'User-agent':'Mozilla/5.0',
    }
    for j in range(1,50):
        link='https://weibo.cn/comment/IAx26EVe9?uid=5626136031&rl=1&rand=850778&page='
        link1=link+str(j)
        try:
            r=s.get(link1,headers=headers)
            r.raise_for_status()
            #r.encoding=r.apparent_encoding
            soup=BeautifulSoup(r.text,'lxml')
            #print(soup.prettify())
            span=soup.find_all('span',class_='ctt')
            for i in span:
                with open(root,'a',encoding='utf-8') as f:
                    f.write(i.text+'\n')
        except:
            print('crawl woring!')
        print('第 %d 篇评论爬取完成' %j)
        time.sleep(random.random()*5)

if __name__=='__main__':
    #login_weibo()
    #get_comments()
    #cutword()
    ciyun()
```

![03292](/img/03292.png)








