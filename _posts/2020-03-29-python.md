---
layout: post
title: 爬取微博评论并做成词云
date: 2020-03-29
categories: blog
tags: [python爬虫]
description: 语言
---

### hello .de
今天做了很久就该做的事，就是换域名。换好了还是和去年4月份第一次配置好.online一样激动。

再见，.online 。你好，.de

### 准备
网上了解到微博的网页版的反爬程度不亚于淘宝，遂放弃网页版的爬取。<br>
了解到微博居然在电脑上有手机版，网址：https://www.weibo.cn/，而且页面简单，少辽很多css和js，我就试了试，居然评论都不是动态的，也就是说就可以直接爬取页面，解析评论就行了。<br>
有一点就是，未登录的话，只能爬取前5条评论，也就是说要模拟登陆微博，不过手机版微博的登陆形式跟豆瓣实际上一样，去找到原始登陆地址，保留cookie，登陆就行。

### 思路和遇到的问题解决
1. 首先模拟登陆微博，方式和登陆豆瓣一样
2. 登陆后，观察每一页评论url的api，然后批量爬取。不过第一次爬取的时候是乱码的，更改encoding就行了。
3. 保存在txt文件后，用jieba分词切割为词语
4. wordcloud库做成词云

## 步骤

### 登陆微博

```
def login_weibo():
    link="https://passport.weibo.cn/sso/login"
    headers={
        'User-agent':'Mozilla/5.0',
        'Referer':'https://passport.weibo.cn/signin/login?entry=mweibo&r=https%3A%2F%2Fweibo.cn%2F%3Fsudaref%3Dlogin.sina.com.cn%26display%3D0%26retcode%3D6102&backTitle=%CE%A2%B2%A9&vt=&sudaref=weibo.cn&display=0&retcode=6102'
    }
    data={
        'username':'1531045xxxx',
        'password':'zzhlzf20xxxxx',
        'entry': 'weibo',
        'savestate': '1',
        'mainpageflag':'1'
    }
    try:
        r=s.post(link,headers=headers,data=data)
        r.raise_for_status()
    except:
        print('登陆失败')
        return 0
```

### 批量爬取，并存于txt文件
```
#得到评论
def get_comments():
    #如果存在文件 移除
    if os.path.exists(root):
        os.remove(root)
    headers={
        'User-agent':'Mozilla/5.0',
    }
    for j in range(1,50):
        link='https://weibo.cn/comment/IAx26EVe9?uid=5626136031&rl=1&rand=850778&page='
        link1=link+str(j)
        try:
            r=s.get(link1,headers=headers)
            r.raise_for_status()
            #r.encoding=r.apparent_encoding
            soup=BeautifulSoup(r.text,'lxml')
            #print(soup.prettify())
            span=soup.find_all('span',class_='ctt')
            for i in span:
                with open(root,'a',encoding='utf-8') as f:
                    f.write(i.text+'\n')
        except:
            print('crawl woring!')
        print('第 %d 篇评论爬取完成' %j)
        time.sleep(random.random()*5)
```

### jieba分词切割

```
#jieba分词
def cutword():
    with open(root,encoding='utf-8') as f:
        string=f.read()
        for ch in'!"#$%&()*+,-./:;：，！<=>?@[\\]^_‘？{|}【】￥~0123456789。（）“”《》<>……；·~':
            string=string.replace(ch,' ')
        string=re.sub('[A-Za-z]','',string)
        wl=jieba.cut(string)
        wl=' '.join(wl)
       # print(wl)
        return wl
```

### 生成词云

```
#生成词云
def ciyun():
    path='C:\\Users\\92507\\Desktop\\weibo.jpg'
    font='simsun.ttc'
    img='boy.jpg'
    mask=np.array(Image.open(img))
    #词云参数配置
    stopword=['回复','还有','不是','这个','那么','可以']
    wl=WordCloud(width=1000,height=700,scale=4,random_state=42,mask=mask,
                 max_words=200,max_font_size=50,font_path=font,background_color='white',stopwords=stopword)
    wl.generate(cutword())
    plot.imshow(wl,interpolation="bilinear")
    plot.axis('off')
    plot.figure()
    plot.show()
    wl.to_file(path)
```

### 完整代码及成果

```
#!/usr/bin/env python
# -*-coding:utf-8-*-
import re
import time
import requests
from bs4 import BeautifulSoup
import  json
import os
import time
import random
import jieba
import numpy as np
import matplotlib.pyplot as plot
from wordcloud import WordCloud
from PIL import Image
s=requests.Session()
root="C:\\Users\\92507\\Desktop\\weibocomments.txt"
#jieba分词
def cutword():
    with open(root,encoding='utf-8') as f:
        string=f.read()
        for ch in'!"#$%&()*+,-./:;：，！<=>?@[\\]^_‘？{|}【】￥~0123456789。（）“”《》<>……；·~':
            string=string.replace(ch,' ')
        string=re.sub('[A-Za-z]','',string)
        wl=jieba.cut(string)
        wl=' '.join(wl)
       # print(wl)
        return wl

#生成词云
def ciyun():
    path='C:\\Users\\92507\\Desktop\\weibo.jpg'
    font='simsun.ttc'
    img='boy.jpg'
    mask=np.array(Image.open(img))
    #词云参数配置
    stopword=['回复','还有','不是','这个','那么','可以']
    wl=WordCloud(width=1000,height=700,scale=4,random_state=42,mask=mask,
                 max_words=200,max_font_size=50,font_path=font,background_color='white',stopwords=stopword)
    wl.generate(cutword())
    plot.imshow(wl,interpolation="bilinear")
    plot.axis('off')
    plot.figure()
    plot.show()
    wl.to_file(path)
#登陆微博
def login_weibo():
    link="https://passport.weibo.cn/sso/login"
    headers={
        'User-agent':'Mozilla/5.0',
        'Referer':'https://passport.weibo.cn/signin/login?entry=mweibo&r=https%3A%2F%2Fweibo.cn%2F%3Fsudaref%3Dlogin.sina.com.cn%26display%3D0%26retcode%3D6102&backTitle=%CE%A2%B2%A9&vt=&sudaref=weibo.cn&display=0&retcode=6102'
    }
    data={
        'username':'15310451436',
        'password':'zzhlzf20000408',
        'entry': 'weibo',
        'savestate': '1',
        'mainpageflag':'1'
    }
    try:
        r=s.post(link,headers=headers,data=data)
        r.raise_for_status()
    except:
        print('登陆失败')
        return 0
#得到评论
def get_comments():
    #如果存在文件 移除
    if os.path.exists(root):
        os.remove(root)
    headers={
        'User-agent':'Mozilla/5.0',
    }
    for j in range(1,50):
        link='https://weibo.cn/comment/IAx26EVe9?uid=5626136031&rl=1&rand=850778&page='
        link1=link+str(j)
        try:
            r=s.get(link1,headers=headers)
            r.raise_for_status()
            #r.encoding=r.apparent_encoding
            soup=BeautifulSoup(r.text,'lxml')
            #print(soup.prettify())
            span=soup.find_all('span',class_='ctt')
            for i in span:
                with open(root,'a',encoding='utf-8') as f:
                    f.write(i.text+'\n')
        except:
            print('crawl woring!')
        print('第 %d 篇评论爬取完成' %j)
        time.sleep(random.random()*5)

if __name__=='__main__':
    #login_weibo()
    #get_comments()
    #cutword()
    ciyun()
```










